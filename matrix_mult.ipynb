{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXNz4F8aMnQo",
        "outputId": "31a225f4-6d7b-485f-b331-9eae4421c00e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Apr  2 01:27:37 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   47C    P8             11W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ws5X62mSMa2U",
        "outputId": "21a576a5-4bfc-4e69-e908-ba4cdc870466"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "!pip install ninja --quiet\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "drWiSzpMMa2V",
        "outputId": "5f219866-62d1-4812-e968-8e36e70d0730"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "torch.Size([512, 512])\n",
            "tensor([[22.1786, 22.0294, 21.5798,  ..., 21.9982, 23.1771, 19.9314],\n",
            "        [23.5077, 24.0081, 23.2886,  ..., 22.0258, 23.5319, 22.3493],\n",
            "        [28.5079, 29.3412, 28.6804,  ..., 27.7836, 27.1417, 26.5278],\n",
            "        ...,\n",
            "        [25.2792, 27.0707, 25.5258,  ..., 25.2689, 26.8388, 25.8101],\n",
            "        [24.1430, 26.2120, 25.5903,  ..., 21.6043, 25.7355, 23.9221],\n",
            "        [26.4715, 27.9845, 25.1181,  ..., 24.7405, 26.0588, 24.4135]],\n",
            "       device='cuda:0')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No modifications detected for re-loaded extension module matrix_dot_product_v1, skipping build step...\n",
            "Loading extension module matrix_dot_product_v1...\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import torch\n",
        "from torch.utils.cpp_extension import load_inline\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "build_directory = './cuda_build'\n",
        "if os.path.exists(build_directory):\n",
        "    shutil.rmtree(build_directory)\n",
        "if not os.path.exists(build_directory):\n",
        "    os.makedirs(build_directory)\n",
        "\n",
        "def compile_extension():\n",
        "    cuda_source = Path(\"/content/drive/MyDrive/Cuda_Learning/kernels/matrix_mult.cu\").read_text()\n",
        "    cpp_source = \"torch::Tensor matrix_mult(torch::Tensor matrix_a, torch::Tensor matrix_b);\"\n",
        "\n",
        "    # Load the CUDA kernel as a PyTorch extension\n",
        "    dot_product_extension = load_inline(\n",
        "        name=\"matrix_dot_product_v1\",\n",
        "        cpp_sources=cpp_source,\n",
        "        cuda_sources=cuda_source,\n",
        "        functions=[\"matrix_mult\"],\n",
        "        with_cuda=True,\n",
        "        extra_cuda_cflags=[\"-O2\"],\n",
        "        verbose=True,\n",
        "        build_directory=build_directory,\n",
        "    )\n",
        "    return dot_product_extension\n",
        "\n",
        "kernel = compile_extension()\n",
        "\n",
        "def _main():\n",
        "    matrix_a = torch.rand(512, 100, device='cuda')\n",
        "    matrix_b = torch.rand(100, 512, device='cuda')\n",
        "\n",
        "    # Perform matrix multiplication using the CUDA kernel\n",
        "    result_cuda = kernel.matrix_mult(matrix_a, matrix_b)\n",
        "    # Perform matrix multiplication using PyTorch for comparison\n",
        "    result_pytorch = torch.matmul(matrix_a, matrix_b)\n",
        "\n",
        "    print(torch.allclose(result_cuda, result_pytorch))\n",
        "    print(result_cuda.shape)\n",
        "    print(result_cuda)\n",
        "\n",
        "\n",
        "_main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jax.lib.xla_bridge as xb\n",
        "print(xb.get_backend().platform)\n",
        "import os\n",
        "os.environ['JAX_PLATFORM_NAME'] = 'gpu'"
      ],
      "metadata": {
        "id": "kldi3KKHIHOO",
        "outputId": "09b3c168-6504-4e6a-ce27-4496a2cbae81",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import time\n",
        "\n",
        "def benchmark_matrix_mult(size):\n",
        "    \"\"\"Benchmarks matrix multiplication for different methods.\"\"\"\n",
        "\n",
        "    # Create input matrices\n",
        "    # By default, np.random.rand creates arrays with the float64 data type (double-precision floating-point numbers).\n",
        "    matrix_a_np = np.random.rand(size, size).astype(np.float32)\n",
        "    matrix_b_np = np.random.rand(size, size).astype(np.float32)\n",
        "\n",
        "    matrix_a_torch_cpu = torch.from_numpy(matrix_a_np).cpu()\n",
        "    matrix_b_torch_cpu = torch.from_numpy(matrix_b_np).cpu()\n",
        "\n",
        "    matrix_a_torch_cuda = torch.from_numpy(matrix_a_np).cuda()\n",
        "    matrix_b_torch_cuda = torch.from_numpy(matrix_b_np).cuda()\n",
        "\n",
        "    matrix_a_jax = jnp.array(matrix_a_np)\n",
        "    matrix_b_jax = jnp.array(matrix_b_np)\n",
        "\n",
        "    # Custom CUDA kernel\n",
        "    start_time = time.time()\n",
        "    result_cuda = kernel.matrix_mult(matrix_a_torch_cuda, matrix_b_torch_cuda)\n",
        "    cuda_time = time.time() - start_time\n",
        "\n",
        "    # NumPy\n",
        "    start_time = time.time()\n",
        "    result_numpy = np.matmul(matrix_a_np, matrix_b_np)\n",
        "    numpy_time = time.time() - start_time\n",
        "\n",
        "    # PyTorch CPU\n",
        "    start_time = time.time()\n",
        "    result_pytorch_cpu = torch.matmul(matrix_a_torch_cpu, matrix_b_torch_cpu)\n",
        "    pytorch_cpu_time = time.time() - start_time\n",
        "\n",
        "    # PyTorch CUDA\n",
        "    start_time = time.time()\n",
        "    result_pytorch_cuda = torch.matmul(matrix_a_torch_cuda, matrix_b_torch_cuda)\n",
        "    pytorch_cuda_time = time.time() - start_time\n",
        "\n",
        "\n",
        "    # JAX\n",
        "    # JAX uses JIT compilation to optimize computations.\n",
        "    #  The first time a function is executed, JAX traces it and compiles an optimized version. Subsequent executions of the same function will be faster.\n",
        "    start_time = time.time()\n",
        "    result_jax = jnp.matmul(matrix_a_jax, matrix_b_jax)\n",
        "    jax_time = time.time() - start_time\n",
        "\n",
        "    # Print results\n",
        "    print(f\"Matrix size: {size}x{size}\")\n",
        "    print(f\"Custom CUDA kernel: {cuda_time:.4f} seconds\")\n",
        "    print(f\"NumPy: {numpy_time:.4f} seconds\")\n",
        "    print(f\"PyTorch CPU: {pytorch_cpu_time:.4f} seconds\")\n",
        "    print(f\"PyTorch CUDA: {pytorch_cuda_time:.4f} seconds\")\n",
        "    print(f\"JAX: {jax_time:.4f} seconds\")\n",
        "\n",
        "benchmark_matrix_mult(2222)"
      ],
      "metadata": {
        "id": "SRczxsTSFFr5",
        "outputId": "60bbc8d7-59fa-43b3-c912-60c75c19502f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matrix size: 2222x2222\n",
            "Custom CUDA kernel: 0.0002 seconds\n",
            "NumPy: 0.4482 seconds\n",
            "PyTorch CPU: 0.3974 seconds\n",
            "PyTorch CUDA: 0.0003 seconds\n",
            "JAX: 0.0007 seconds\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}